---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---

Years: [2022](#2022)・[2019](#2019)・[2018](#2018)・[2017](#2017)


## 2022 ##

---

<details>
<summary> 
<strong style="color:#52adc8">Search-Based Testing of Reinforcement Learning</strong> <br>
Tappler, M., <strong>Cano Córdoba, F.</strong>, Aichernig, B. K., & Könighofer, B. 
International Joint Conference of Artificial Intelligence (<strong>IJCAI</strong>) 2022.
<a href="https://arxiv.org/pdf/2205.04887">
<img src="./../images/pdf-svg.svg" width=16em title="pdf"/>
</a>
</summary>
<br>
<div style="margin-left: 2em">
<strong> Abstract: </strong>
Evaluation of deep reinforcement learning (RL) is inherently challenging. Especially the opaqueness of learned policies and the stochastic nature of both agents and environments make testing the behavior of deep RL agents difficult. We present a search-based testing framework that enables a wide range of novel analysis capabilities for evaluating the safety and performance of deep RL agents. For safety testing, our framework utilizes a search algorithm that searches for a reference trace that solves the RL task. The backtracking states of the search, called boundary states, pose safety-critical situations. We create safety test-suites that evaluate how well the RL agent escapes safety-critical situations near these boundary states. For robust performance testing, we create a diverse set of traces via fuzz testing. These fuzz traces are used to bring the agent into a wide variety of potentially unknown states from which the average performance of the agent is compared to the average performance of the fuzz traces. We apply our search-based testing approach on RL for Nintendo's Super Mario Bros.

<br>
<br>
<details><summary>BibTex:</summary>
<pre>
@article{tappler2022search,
  title={Search-Based Testing of Reinforcement Learning},
  author={Tappler, Martin and Cano C{\'o}rdoba, Filip and Aichernig,   Bernhard K and K{\"o}nighofer, Bettina},
  journal={arXiv e-prints},
  pages={arXiv--2205},
  year={2022}
}
</pre>
</details>
</div>
</details>

---

## 2019 ##

---

<details>
<summary> 
<strong style="color:#52adc8">An Introduction to Polytope Theory through Ehrhart's Theorem</strong> <br>
<strong>Cano Córdoba, F.</strong>
Master Thesis, 2019.
<a href="https://upcommons.upc.edu/handle/2117/171328">
<img src="./../images/pdf-svg.svg" width=16em title="pdf"/>
</a>
</summary>
<br>
<div style="margin-left: 2em">
<strong> Abstract: </strong>
A classic introduction to polytope theory is presented, serving as the foundation to develop more advanced theoretical tools, namely the algebra of polyhedra and the use of valuations. The main theoretical objective is the construction of the so called Berline-Vergne valuation. Most of the theoretical development is aimed towards this goal. A little survey on Ehrhart positivity is presented, as well as some calculations that lead to conjecture that generalized permutohedra have positive coefficients in their Ehrhart polynomials. Throughout the thesis three different proofs of Ehrhart's theorem are presented, as an application of the new techniques developed.

<br>
<br>
<details><summary>BibTex:</summary>
<pre>
@mastersthesis{cano2019introduction,
  title={An Introduction to Polytope Theory through Ehrhart's Theorem},
  author={Cano C{\'o}rdoba, Filip},
  type={M.S. thesis},
  year={2019},
  school={Universitat Polit{\`e}cnica de Catalunya}
}
</pre>
</details>
</div>
</details>

---

## 2018 ##

---

<details>
<summary> 
<strong style="color:#52adc8">Theoretical study of artificial neural networks</strong> <br>
<strong>Cano Córdoba, F.</strong> Bachelor Thesis, 2018.
<a href="https://upcommons.upc.edu/bitstream/handle/2117/121051/memoria.pdf">
<img src="./../images/pdf-svg.svg" width=16em title="pdf"/>
</a>
</summary>
<br>
<div style="margin-left: 2em">
<strong> Abstract: </strong>
The basic structure and definitions of artificial neural networks are exposed, as an introduction to Machine Learning algorithms. The theoretical description is emphasized and representation power of both shallow and deep networks is studied, proving the so called \textit{Universality Theorem}. Then the properties and limitations of learning algorithms are studied. More specifically, the \textit{No Free Lunch Theorem} is presented and proven, and then some recent approaches to the open problem of convergence of Stochastic Gradient Descent applied to neural networks are presented. Finally, a concept of forgetting in neural networks is introduced and some results on this model are given throughout the thesis.

<br>
<br>
<details><summary>BibTex:</summary>
<pre>
@mastersthesis{cano2018theoretical,
  title={Theoretical study of artificial neural networks},
  author={Cano C{\'o}rdoba, Felipe},
  type={B.S. thesis},
  year={2018},
  school={Universitat Polit{\`e}cnica de Catalunya}
}
</pre>
</details>
</div>
</details>

---

## 2017 ##

---

<details>
<summary> 
<strong style="color:#52adc8">Theory of Intelligence with Forgetting</strong> <br>
<strong>Cano Córdoba, F.</strong> , Sarma S, Subirana B.
Center For Brain Minds and Machines (CBMM) Memo no. 71, 2017.
<a href="https://dspace.mit.edu/bitstream/handle/1721.1/113608/CBMM-Memo-071.pdf">
<img src="./../images/pdf-svg.svg" width=16em title="pdf"/>
</a>
</summary>
<br>
<div style="margin-left: 2em">
<strong> Abstract: </strong>
In [42] we suggested that any memory stored in the human/animal brain is forgotten following the Ebingghaus curve – in this follow-on paper, we define a novel algebraic structure, a Forgetting Neural Network, as a simple mathematical model based on assuming parameters of a neuron in a neural network are forgotten using the Ebbinghaus forgetting curve. We model neural networks in Sobolev spaces using [35] as our departure point and demonstrate four novel theorems of Forgetting Neural Networks: theorem of non-instantaneous forgetting, theorem of universal forgetting, curse of forgetting theorem, and center of mass theorem. We also proof the novel decreasing inference theorem which we feel is relevant beyond Ebbinghaus forgetting: compositional deep neural networks cannot arbitrarily combine low level “features” – meaning only certain arrangements of features calculated in intermediate levels can show up in higher levels. This proof leads us to present the possibly most efficient representation of neural networks’ “minimal polynomial basis layer” (MPBL) since our basis construct can generate n polynomials of order m using only 2m + 1 + n neurons. As we briefly discuss in the conclusion, there are about 10 similarities between forgetting neural networks and human forgetting and our research elicits more questions than it answers and may have implications for neuroscience research including our understanding of how babies learn (or, perhaps, forget), including what we call the baby forgetting conjecture.

<br>
<br>
<details><summary>BibTex:</summary>
<pre>
@techreport{cano2017theory,
  title={Theory of intelligence with forgetting: Mathematical theorems explaining human universal forgetting using “forgetting neural networks”},
  author={Cano-C{\'o}rdoba, Felipe and Sarma, Sanjay and Subirana, Brian},
  year={2017},
  institution={Center for Brains, Minds and Machines (CBMM)}
}
</pre>
</details>
</div>
</details>
